{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb NYC Rental Prices Analysis\n",
    "\n",
    "**Author:** David Graham\n",
    "\n",
    "**Dataset:** Airbnb NYC 2019 Listings\n",
    "\n",
    "## Project Description\n",
    "\n",
    "This project performs an exploratory data analysis on the Airbnb NYC 2019 dataset to uncover patterns and relationships between listing features and rental prices. Through data visualization and statistical analysis, we aim to identify key factors that influence pricing and discover meaningful insights about the short-term rental market in New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AB_NYC_2019.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def handle_missing_values(dataframe, numeric_strategy='median', text_strategy='Unknown'):\n    \"\"\"\n    Handle missing values in a DataFrame by filling numeric columns with a \n    specified strategy and text columns with a placeholder value.\n    \n    Parameters:\n    -----------\n    dataframe : pd.DataFrame\n        The input DataFrame with missing values\n    numeric_strategy : str, default='median'\n        Strategy for filling numeric columns ('median', 'mean', or 'zero')\n    text_strategy : str, default='Unknown'\n        Value to fill missing text/object columns\n    \n    Returns:\n    --------\n    pd.DataFrame\n        DataFrame with missing values handled\n    \"\"\"\n    df_clean = dataframe.copy()\n    \n    for column in df_clean.columns:\n        if df_clean[column].isnull().sum() > 0:\n            if df_clean[column].dtype in ['float64', 'int64']:\n                if numeric_strategy == 'median':\n                    df_clean[column] = df_clean[column].fillna(df_clean[column].median())\n                elif numeric_strategy == 'mean':\n                    df_clean[column] = df_clean[column].fillna(df_clean[column].mean())\n                else:\n                    df_clean[column] = df_clean[column].fillna(0)\n            else:\n                df_clean[column] = df_clean[column].fillna(text_strategy)\n    \n    return df_clean\n\n\ndef remove_price_outliers(dataframe, price_column='price', lower_percentile=1, upper_percentile=99):\n    \"\"\"\n    Remove outliers from the price column based on percentile thresholds.\n    This helps eliminate extreme values that could skew analysis results.\n    \n    Parameters:\n    -----------\n    dataframe : pd.DataFrame\n        The input DataFrame containing price data\n    price_column : str, default='price'\n        Name of the column containing price values\n    lower_percentile : int, default=1\n        Lower percentile threshold (removes values below this)\n    upper_percentile : int, default=99\n        Upper percentile threshold (removes values above this)\n    \n    Returns:\n    --------\n    pd.DataFrame\n        DataFrame with price outliers removed\n    \"\"\"\n    df_clean = dataframe.copy()\n    \n    lower_bound = df_clean[price_column].quantile(lower_percentile / 100)\n    upper_bound = df_clean[price_column].quantile(upper_percentile / 100)\n    \n    original_count = len(df_clean)\n    df_clean = df_clean[(df_clean[price_column] >= lower_bound) & \n                        (df_clean[price_column] <= upper_bound)]\n    \n    removed_count = original_count - len(df_clean)\n    print(f\"Removed {removed_count} outliers ({removed_count/original_count*100:.2f}%)\")\n    print(f\"Price range: ${lower_bound:.0f} - ${upper_bound:.0f}\")\n    \n    return df_clean"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before cleaning\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning functions\n",
    "df_clean = handle_missing_values(df)\n",
    "df_clean = remove_price_outliers(df_clean)\n",
    "\n",
    "# Verify cleaning results\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_clean.isnull().sum())\n",
    "print(f\"\\nFinal row count: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Exploratory Data Analysis Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_summary_statistics(dataframe, numeric_only=True):\n    \"\"\"\n    Generate comprehensive summary statistics for a DataFrame including\n    count, mean, std, min, max, and percentiles for numeric columns.\n    \n    Parameters:\n    -----------\n    dataframe : pd.DataFrame\n        The input DataFrame to analyze\n    numeric_only : bool, default=True\n        If True, only include numeric columns in the summary\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Summary statistics including additional metrics like skewness\n    \"\"\"\n    if numeric_only:\n        numeric_df = dataframe.select_dtypes(include=[np.number])\n    else:\n        numeric_df = dataframe\n    \n    summary = numeric_df.describe()\n    \n    # Add additional statistics\n    summary.loc['skew'] = numeric_df.skew()\n    summary.loc['median'] = numeric_df.median()\n    \n    return summary.round(2)\n\n\ndef analyze_by_group(dataframe, group_column, value_column, agg_funcs=['mean', 'median', 'count']):\n    \"\"\"\n    Perform grouped analysis on a DataFrame, calculating aggregate statistics\n    for a value column grouped by a categorical column.\n    \n    Parameters:\n    -----------\n    dataframe : pd.DataFrame\n        The input DataFrame to analyze\n    group_column : str\n        Column name to group by (categorical)\n    value_column : str\n        Column name to calculate statistics for (numeric)\n    agg_funcs : list, default=['mean', 'median', 'count']\n        List of aggregation functions to apply\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Grouped statistics sorted by mean value descending\n    \"\"\"\n    grouped = dataframe.groupby(group_column)[value_column].agg(agg_funcs)\n    grouped = grouped.sort_values(by='mean', ascending=False)\n    \n    return grouped.round(2)\n\n\ndef get_correlation_analysis(dataframe, target_column=None, threshold=0.3):\n    \"\"\"\n    Calculate correlation matrix for numeric columns and optionally\n    highlight correlations with a target column above a threshold.\n    \n    Parameters:\n    -----------\n    dataframe : pd.DataFrame\n        The input DataFrame to analyze\n    target_column : str, optional\n        If provided, show correlations with this column sorted by strength\n    threshold : float, default=0.3\n        Minimum absolute correlation value to highlight\n    \n    Returns:\n    --------\n    pd.DataFrame or pd.Series\n        Full correlation matrix, or correlations with target column\n    \"\"\"\n    numeric_df = dataframe.select_dtypes(include=[np.number])\n    correlation_matrix = numeric_df.corr()\n    \n    if target_column and target_column in correlation_matrix.columns:\n        target_corr = correlation_matrix[target_column].drop(target_column)\n        target_corr = target_corr.sort_values(key=abs, ascending=False)\n        \n        print(f\"Correlations with '{target_column}' (|r| >= {threshold}):\")\n        significant = target_corr[abs(target_corr) >= threshold]\n        return significant.round(3)\n    \n    return correlation_matrix.round(3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Run Exploratory Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Summary Statistics\nprint(\"=== Summary Statistics ===\\n\")\nget_summary_statistics(df_clean)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Grouped Analysis: Price by Neighbourhood Group\nprint(\"=== Price by Borough ===\\n\")\nanalyze_by_group(df_clean, 'neighbourhood_group', 'price')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Grouped Analysis: Price by Room Type\nprint(\"=== Price by Room Type ===\\n\")\nanalyze_by_group(df_clean, 'room_type', 'price')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Correlation Analysis\nprint(\"=== Correlation Analysis ===\\n\")\nget_correlation_analysis(df_clean, target_column='price', threshold=0.1)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}